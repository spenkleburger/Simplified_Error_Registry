# Models | Cursor Docs

Source URL: https://cursor.com/docs/models

---

Get Started
# Models

Cursor supports all frontier coding models from all major model providers.

NameDefault ContextMax ModeCapabilitiesClaude 4.1 Opus-200kAgentThinkingClaude 4.5 Sonnet200k1MAgentThinkingComposer 1200k-AgentGemini 3 Pro200k1MAgentThinkingGPT-5.1272k-AgentThinkingGPT-5.1 Codex272k-AgentThinkingGrok Code256k-AgentThinkingShow more models

## Model pricing

Cursor [plans](/docs/account/pricing) include usage at the model API rates. For example, $20 of included usage on the Pro plan will be consumed based on your model selection and its price.

Usage limits are shown in editor based on your current consumption. All prices are per million tokens.

NameInputCache WriteCache ReadOutputClaude 4.1 Opus$15$18.75$1.5$75Claude 4.5 Sonnet$3$3.75$0.3$15Composer 1$1.25$1.25$0.125$10Gemini 3 Pro$2$2$0.2$12GPT-5.1$1.25$1.25$0.125$10GPT-5.1 Codex$1.25$1.25$0.125$10Grok Code$0.2$0.2$0.02$1.5Show more models

These prices are from the model's API documentation:

[OpenAI Pricing](https://openai.com/api/pricing/)
[Anthropic Pricing](https://www.anthropic.com/pricing#api)
[Google Gemini Pricing](https://ai.google.dev/gemini-api/docs/pricing)
[xAI Pricing](https://docs.x.ai/docs/models)

## Auto

Enabling Auto allows Cursor to select the premium model best fit for the immediate task and with the highest reliability based on current demand. This feature can detect degraded output performance and automatically switch models to resolve it.

## Context windows

A [context window](/learn/context) is the maximum span of tokens (text and code) an LLM can consider at once, including both the input prompt and output generated by the model.

Each chat in Cursor maintains its own context window. The more prompts, attached files, and responses included in a session, the more context is added, filling up the available context window.

Learn more about [working with context](/learn/context) in Cursor.

## Max Mode

Normally, Cursor uses a context window of 200k tokens (~15,000 lines of code). Max Mode extends the context window to the maximum available for a handful of models. This will be a bit slower and more expensive. It is most relevant for Gemini 2.5 Flash, Gemini 2.5 Pro, GPT 4.1, and Grok 4, which have context windows larger than 200k.

## FAQ

### Where are models hosted?