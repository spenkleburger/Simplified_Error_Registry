# ============================================================================
# Environment Variables Template
# ============================================================================
# Copy this file to .env and customize values for your project
# NEVER commit .env to version control - only .env.example is tracked
# ============================================================================

# ============================================================================
# Application Configuration
# ============================================================================

ENVIRONMENT=development
DEBUG=True

# ============================================================================
# Logging Configuration
# ============================================================================

LOG_LEVEL=INFO
LOG_TO_FILE=True
LOG_TO_CONSOLE=True
LOG_DIR=./logs
LOG_FILE_MAX_SIZE=10MB
LOG_RETENTION_DAYS=30

# ============================================================================
# Debugging Configuration
# ============================================================================

VERBOSE=False
SHOW_STACK_TRACE=True

# ============================================================================
# API Keys (Sensitive - Never commit to version control)
# ============================================================================

# API_KEY=your-api-key-here
# OPENAI_API_KEY=your-openai-api-key-here
# ANTHROPIC_API_KEY=your-anthropic-api-key-here


# ============================================================================
# LLM Configuration (Simplified Error Registry)
# ============================================================================
# Configure LLM providers and models for consolidation app tasks
# Supports per-task provider/model selection for optimal cost/performance
# ============================================================================

# Default provider and model (fallback for all tasks)
LLM_PROVIDER=ollama
LLM_MODEL=qwen2.5-coder:14b

# Per-task providers (optional - overrides default provider for specific tasks)
# Allows mixing local (Ollama) and cloud (OpenAI/Anthropic) providers
# LLM_PROVIDER_DEDUPLICATION=ollama
# LLM_PROVIDER_TAGGING=openai
# LLM_PROVIDER_RULE_EXTRACTION=anthropic

# Per-task models (optional - overrides default model for specific tasks)
# LLM_MODEL_DEDUPLICATION=qwen2.5-coder:7b
# LLM_MODEL_TAGGING=gpt-4o-mini
# LLM_MODEL_RULE_EXTRACTION=claude-3-opus-20240229

# Ollama configuration (only if using Ollama)
OLLAMA_BASE_URL=http://localhost:11434

# API Keys (only needed for cloud providers)
# OPENAI_API_KEY=sk-your-openai-api-key-here
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# ============================================================================
# Example Configurations
# ============================================================================

# Example 1: All Local (Ollama)
# LLM_PROVIDER=ollama
# LLM_MODEL=qwen2.5-coder:14b
# LLM_MODEL_DEDUPLICATION=qwen2.5-coder:7b
# LLM_MODEL_TAGGING=qwen2.5-coder:7b
# LLM_MODEL_RULE_EXTRACTION=qwen2.5-coder:14b

# Example 2: Mix Local and Cloud (Recommended)
# LLM_PROVIDER=ollama
# LLM_MODEL=qwen2.5-coder:14b
# LLM_PROVIDER_DEDUPLICATION=ollama
# LLM_MODEL_DEDUPLICATION=qwen2.5-coder:7b
# LLM_PROVIDER_TAGGING=ollama
# LLM_MODEL_TAGGING=qwen2.5-coder:7b
# LLM_PROVIDER_RULE_EXTRACTION=openai
# LLM_MODEL_RULE_EXTRACTION=gpt-4
# OPENAI_API_KEY=sk-your-openai-api-key-here

# Example 3: All Cloud (OpenAI)
# LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-your-openai-api-key-here
# LLM_MODEL=gpt-4o-mini
# LLM_MODEL_DEDUPLICATION=gpt-4o-mini
# LLM_MODEL_TAGGING=gpt-4o-mini
# LLM_MODEL_RULE_EXTRACTION=gpt-4

# ============================================================================

# API_KEY=your-api-key-here
# OPENAI_API_KEY=your-openai-api-key-here

# ============================================================================
# Database Configuration
# ============================================================================

DATABASE_URL=postgresql://user:password@localhost:5432/dbname
DB_NAME=mydb
DB_USER=user
DB_PASSWORD=password

# ============================================================================
# Port Configuration
# ============================================================================

API_PORT=8000
WEB_PORT=3000
DATABASE_PORT=5432

# ============================================================================
# Paths
# ============================================================================

DATA_DIR=./data

# ============================================================================
# AI Infrastructure & GER Config
# ============================================================================

OLLAMA_HOST=http://host.docker.internal:11434
REGISTRY_PATH=/app/ger_knowledge

# ============================================================================


# ============================================================================
# LLM Configuration (Simplified Error Registry)
# ============================================================================
# Configure LLM providers and models for consolidation app tasks
# Supports per-task provider/model selection for optimal cost/performance
# ============================================================================

# Default provider and model (fallback for all tasks)
LLM_PROVIDER=ollama
LLM_MODEL=qwen2.5-coder:14b

# Per-task providers (optional - overrides default provider for specific tasks)
# Allows mixing local (Ollama) and cloud (OpenAI/Anthropic) providers
# LLM_PROVIDER_DEDUPLICATION=ollama
# LLM_PROVIDER_TAGGING=openai
# LLM_PROVIDER_RULE_EXTRACTION=anthropic

# Per-task models (optional - overrides default model for specific tasks)
# LLM_MODEL_DEDUPLICATION=qwen2.5-coder:7b
# LLM_MODEL_TAGGING=gpt-4o-mini
# LLM_MODEL_RULE_EXTRACTION=claude-3-opus-20240229

# Ollama configuration (only if using Ollama)
OLLAMA_BASE_URL=http://localhost:11434

# API Keys (only needed for cloud providers - uncomment and set as needed)
# OPENAI_API_KEY=sk-your-openai-api-key-here
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# ============================================================================
# Example Configurations
# ============================================================================

# Example 1: All Local (Ollama)
# LLM_PROVIDER=ollama
# LLM_MODEL=qwen2.5-coder:14b
# LLM_MODEL_DEDUPLICATION=qwen2.5-coder:7b
# LLM_MODEL_TAGGING=qwen2.5-coder:7b
# LLM_MODEL_RULE_EXTRACTION=qwen2.5-coder:14b

# Example 2: Mix Local and Cloud (Recommended for cost optimization)
# LLM_PROVIDER=ollama
# LLM_MODEL=qwen2.5-coder:14b
# LLM_PROVIDER_DEDUPLICATION=ollama
# LLM_MODEL_DEDUPLICATION=qwen2.5-coder:7b
# LLM_PROVIDER_TAGGING=ollama
# LLM_MODEL_TAGGING=qwen2.5-coder:7b
# LLM_PROVIDER_RULE_EXTRACTION=openai
# LLM_MODEL_RULE_EXTRACTION=gpt-4
# OPENAI_API_KEY=sk-your-openai-api-key-here

# Example 3: All Cloud (OpenAI)
# LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-your-openai-api-key-here
# LLM_MODEL=gpt-4o-mini
# LLM_MODEL_DEDUPLICATION=gpt-4o-mini
# LLM_MODEL_TAGGING=gpt-4o-mini
# LLM_MODEL_RULE_EXTRACTION=gpt-4

# Example 4: Mix All Three Providers
# LLM_PROVIDER=ollama
# LLM_MODEL=qwen2.5-coder:14b
# LLM_PROVIDER_DEDUPLICATION=ollama
# LLM_MODEL_DEDUPLICATION=qwen2.5-coder:7b
# LLM_PROVIDER_TAGGING=openai
# LLM_MODEL_TAGGING=gpt-4o-mini
# OPENAI_API_KEY=sk-your-openai-api-key-here
# LLM_PROVIDER_RULE_EXTRACTION=anthropic
# LLM_MODEL_RULE_EXTRACTION=claude-3-opus-20240229
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

